from __future__ import division
from sklearn.feature_extraction.text import TfidfVectorizer
from operator import itemgetter
import os
import codecs

def read_files_text(path):
    content_all_files = []
    for file in os.listdir(path):
        if file.endswith(".txt"):
            content = codecs.open(os.path.join(path, file), encoding='utf-8', errors='replace')
            content_all_files.append(content.read())
    return content_all_files


def score_candidates(filename, train):

    file_name = open(filename)
    vectorizer = TfidfVectorizer(use_idf=False, stop_words='english', ngram_range=(1, 3))
    vectorizer.fit_transform(train)
    my_doc_tfidf = vectorizer.transform(file_name)

    feature_names = vectorizer.get_feature_names()
    dense = my_doc_tfidf.todense()
    densed_idf = dense[0].tolist()[0]
    scores = zip(range(0, len(densed_idf)), densed_idf)
    feature_sorted = sorted(scores, key=itemgetter(1))[-5:]

    feature_name_sorted = []
    key_phrases_dic = {}
    key_phrases_values = {}

    for pair in feature_sorted:
        feature_name_sorted.append(feature_names[pair[0]])
        key_phrases_values[feature_names[pair[0]]] = pair[1]

    for word in feature_name_sorted:
        key_phrases_dic[word] = 0
    #print key_phrases_values
    return key_phrases_dic, key_phrases_values, feature_name_sorted

def phrases_spliter(filename):
    list_phrases = []
    with open(filename, 'r') as file:
        all_phrases = file.read().strip().split('.')
        for words in all_phrases:
            list_phrases.append(words.split())
        return list_phrases

def check_keys(key_word, graph):
    for key in graph:
        if key_word in graph:
            return True
    return False

def create_graph(list_phrases, key_phrases_dic):
    graph = {}
    list_keywords_phrase = []
    list_keywords_phrase_bi = []
    list_keywords_phrase_tri = []


    for phrase in list_phrases:

        sentence = [x.lower() for x in phrase]
        bigrams = zip(sentence, sentence[1:])
        trigrams = zip(sentence, sentence[1:], sentence[2:])

        for word in sentence:
            if word in key_phrases_dic:
                list_keywords_phrase.append(word)
                if check_keys(word, graph) == False:
                    graph[word] = list_keywords_phrase

        for j in bigrams:
            big_str = j[0] + ' ' + j[1]
            if big_str in key_phrases_dic:
                list_keywords_phrase_bi.append(big_str)
                if check_keys(big_str, graph) == False:
                    graph[big_str] = list_keywords_phrase_bi

        for i in trigrams:
            trig_str = i[0] + ' ' + i[1] + ' ' + i[2]
            if trig_str in key_phrases_dic:
                list_keywords_phrase_tri.append(trig_str)
                if check_keys(trig_str, graph) == False:
                    graph[trig_str] = list_keywords_phrase_tri

        list_keywords_phrase = []
        list_keywords_phrase_bi = []
        list_keywords_phrase_tri = []
    return graph


def keyPhraseOcurrenceLine(filename, key_phrases_list):
    key_phrases_occurrence_phrase = {}
    nr_phrase = 0
    with open(filename, 'r') as file:
        all_phrases = file.read().strip().split('.')
        for phrase in all_phrases:
            line_aux = phrase.split()
            nr_phrase += 1
            for word in line_aux:
                if word in key_phrases_list and word not in key_phrases_occurrence_phrase:
                    key_phrases_occurrence_phrase[word] = [nr_phrase]
                if word in key_phrases_list and nr_phrase not in key_phrases_occurrence_phrase.get(word, []):
                    key_phrases_occurrence_phrase.get(word, []).append(nr_phrase)
    return key_phrases_occurrence_phrase

#Topic one
def calculatePositionPrior(key_phrases_occurrence_phrase, node, flag):
    probability = 0
    if flag != 0:
        for keyphrase in key_phrases_occurrence_phrase:
            for nr_phrase in key_phrases_occurrence_phrase.get(node, []):
                probability += nr_phrase
    else:
        if node in key_phrases_occurrence_phrase:
            for nr_phrase in key_phrases_occurrence_phrase.get(node, []):
                print nr_phrase
                probability += nr_phrase
    return probability

#Topic three
def calculateOcurrenceWeight(graph, key_phrase_pk, key_phrase_pj):
    weight_value = 1.0
    for key_phrase in graph:
        if key_phrase_pk == key_phrase:
            for word in graph[key_phrase_pk]:
                if word == key_phrase_pj:
                    weight_value += 1
    return weight_value

#Topic two
def calculateTfidfPrior(graph,key_phrases_pi,key_phrases_values, flag):
    probability = 1.0
    if flag != 0:
        for keyphrase in key_phrases_values:
            if keyphrase in graph:
                probability += key_phrases_values[keyphrase]
        return probability
    probability *= key_phrases_values[key_phrases_pi]
    return probability

def sumPrior(graph, key_phrases_values):
    result_sum = 0
    for keyphrase in key_phrases_values:
        if keyphrase in graph:
            result_sum += key_phrases_values[keyphrase]
    return result_sum


def sumDenominator(graph, key_phrase_pk, key_phrase_pj):
    res_weight = 1.0
    for key_phrase in graph:
        if key_phrase_pj in graph:
            for key_phrase_pj_value in graph[key_phrase_pj]:
                if key_phrase_pj_value == key_phrase_pk:
                    res_weight += calculateOcurrenceWeight(graph, key_phrase_pk, key_phrase_pj)
    return res_weight


def sum2Parcel(key_phrase_pi, key_phrase_pk, key_phrase_pj, graph):
    sum_res = 1.0
    for key_phrase in graph:
        if key_phrase_pi in graph:
            for key_phrase_pi_value in graph[key_phrase_pi]:
                if key_phrase_pi_value == key_phrase_pj:
                    sum_res = (0.15*calculateOcurrenceWeight(graph, key_phrase_pk, key_phrase_pi))/(sumDenominator(graph, key_phrase_pk, key_phrase_pj))
    return sum_res

def sum1Parcel(graph, key_phrase_pi, key_phrases_values):
    return (0.15 * (calculateTfidfPrior(graph, key_phrase_pi, key_phrases_values, 0) / sumPrior(graph, key_phrases_values))) + (1-0.15)


def pageRankExtended(graph, key_phrases_values, key_phrase_pi, key_phrase_pk, key_phrase_pj):

    result_parcel_1 = 1.0 * sum1Parcel(graph, key_phrase_pi, key_phrases_values)  #working
    result_parcel_2 = 1.0 * sum2Parcel(key_phrase_pi, key_phrase_pk, key_phrase_pj, graph)

    print "Page Rank result", result_parcel_1 * result_parcel_2
    return result_parcel_1 * result_parcel_2 


def initializer():

    train_data = read_files_text('NLM_500/documents/')
    key_phrase_dic, key_phrases_values, key_phrases_list = score_candidates('teste', train_data)
    list_phrases = phrases_spliter('teste')
    graph = create_graph(list_phrases, key_phrase_dic)
    #key_phrases_occurrence_phrase = keyPhraseOcurrenceLine('teste', key_phrases_list)
    #calculatePositionPrior(key_phrases_occurrence_phrase, "anthony", 1)
    #calculateOcurrenceWeight(graph, "john", "home")
    pageRankExtended(graph, key_phrases_values, "home", "does", "anthony")

initializer()