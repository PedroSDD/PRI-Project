from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from operator import itemgetter

def score_candidates(filename):

    file_name = open(filename)
    train = fetch_20newsgroups(subset='train')
    vectorizer = TfidfVectorizer(use_idf=False, stop_words='english', ngram_range=(1, 3))
    vectorizer.fit_transform(train.data)
    my_doc_tfidf = vectorizer.transform(file_name)

    feature_names = vectorizer.get_feature_names()
    dense = my_doc_tfidf.todense()
    densed_idf = dense[0].tolist()[0]
    scores = zip(range(0, len(densed_idf)), densed_idf)
    feature_sorted = sorted(scores, key=itemgetter(1))[-5:]

    feature_name_sorted = []
    key_phrases_dic = {}
    key_phrases_values = {}

    for pair in feature_sorted:
        feature_name_sorted.append(feature_names[pair[0]])
        key_phrases_values[feature_names[pair[0]]] = pair[1]

    for word in feature_name_sorted:
        key_phrases_dic[word] = 0

    return key_phrases_dic, key_phrases_values

def phrases_spliter(filename):
    list_phrases = []
    with open(filename, 'r') as file:
        all_phrases = file.read().strip().split('.')
        for words in all_phrases:
            list_phrases.append(words.split())
        return list_phrases

def create_graph(list_phrases, key_phrases_dic):
    graph = {}
    list_keywords_phrase = []
    for phrase in list_phrases:
        for word in phrase:
            if word in key_phrases_dic:
                list_keywords_phrase.append(word)
                graph[word] = list_keywords_phrase
        list_keywords_phrase = []
    return graph

#def calculateWeightDyanmic(graph, key_phrase_pk, key_phrase_pj):


#def calculatePositionPrior()



def calculateTfidfPrior(graph,node,key_phrases_values, flag):
    probability = 1.0
    if flag != 0:
        for keyphrase in key_phrases_values:
            if keyphrase in graph:
                probability += key_phrases_values[keyphrase]
        return probability
    probability *= key_phrases_values[node]
    return probability

def sumPrior(graph, key_phrases_values):
    result_sum = 0
    for keyphrase in key_phrases_values:
        if keyphrase in graph:
            result_sum += key_phrases_values[keyphrase]
    return result_sum


def sumDenominator(graph, key_phrase_pk, key_phrase_pj):
    res_weight = 1.0
    for key_phrase in graph:
        if key_phrase_pj in graph:
            for key_phrase_pj_value in graph[key_phrase_pj]:
                if key_phrase_pj_value == key_phrase_pk:
                    res_weight += 1                             #Calcula o peso.  Querem por outra cena?
    return res_weight


def sum2Parcel(key_phrase_pi, key_phrase_pk, key_phrase_pj, graph):
    sum_res = 1.0
    for key_phrase in graph:
        if key_phrase_pi in graph:
            for key_phrase_pi_value in graph[key_phrase_pi]:
                if key_phrase_pi_value == key_phrase_pj:
                    sum_res = (0.375*0.25)/(sumDenominator(graph, key_phrase_pk, key_phrase_pj))
    return sum_res

def sum1Parcel(graph, key_phrase_pi, key_phrases_values):
    return 0.15 * (calculateStaticPrior(graph, key_phrase_pi, 0) / sumPrior(graph, key_phrases_values)) + (1-0.15)


def pageRankExtended(graph, key_phrases_values, key_phrase_pi, key_phrase_pk, key_phrase_pj):

    result_parcel_1 = 1.0 * sum1Parcel(graph, key_phrase_pi, key_phrases_values)
    result_parcel_2 = 1.0 * sum2Parcel(key_phrase_pi, key_phrase_pk, key_phrase_pj, graph)

    print "Page Rank result", result_parcel_1 * result_parcel_2
    return result_parcel_1 * result_parcel_2 

def initializer():
    key_phrase_dic, key_phrases_values = score_candidates('teste')
    list_phrases = phrases_spliter('teste')
    graph = create_graph(list_phrases, key_phrase_dic)
    pageRankExtended(graph, key_phrases_values, 0.375, "anthony", "anthony", "anthony")

initializer()