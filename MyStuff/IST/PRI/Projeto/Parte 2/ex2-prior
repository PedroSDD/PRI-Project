from __future__ import division
from sklearn.feature_extraction.text import TfidfVectorizer
from operator import itemgetter
import os
import codecs
from os import walk

def read_files_text(path):
    content_all_files = []
    for file in os.listdir(path):
        if file.endswith(".txt"):
            content = codecs.open(os.path.join(path, file), encoding='utf-8', errors='replace')
            content_all_files.append(content.read())
    return content_all_files


def score_candidates(filename, train):
    file_name = codecs.open(filename, encoding='utf-8', errors='replace')
    vectorizer = TfidfVectorizer(use_idf=False, stop_words='english', ngram_range=(1, 3))
    vectorizer.fit_transform(train)
    my_doc_tfidf = vectorizer.transform(file_name)

    feature_names = vectorizer.get_feature_names()
    dense = my_doc_tfidf.todense()
    densed_idf = dense[0].tolist()[0]
    scores = zip(range(0, len(densed_idf)), densed_idf)
    feature_sorted = sorted(scores, key=itemgetter(1))[-5:]

    feature_name_sorted = []
    key_phrases_dic = {}
    key_phrases_values = {}

    for pair in feature_sorted:
        feature_name_sorted.append(feature_names[pair[0]])
        key_phrases_values[feature_names[pair[0]]] = pair[1]

    for word in feature_name_sorted:
        key_phrases_dic[word] = 0
    #print key_phrases_values
    return key_phrases_dic, key_phrases_values, feature_name_sorted

def phrases_spliter(filename):
    list_phrases = []
    with open(filename, 'r') as file:
        all_phrases = file.read().strip().split('.')
        for words in all_phrases:
            list_phrases.append(words.split())
        return list_phrases

def generate_graph(all_edges_list):
    graph = {}
    edges_list=[]
    for line in all_edges_list:
        edges_list = line

        for value in edges_list:
            if value not in graph:
                for other_value in edges_list:
                    if value != other_value:
                        graph.setdefault(value, []).append(other_value)

            if value in graph:
                for other_value in edges_list:
                    if other_value not in graph[value] and value != other_value:
                        graph.setdefault(value, []).append(other_value)

    return graph


def create_graph(list_phrases, key_phrases_dic):
    graph = {}
    list_keywords_phrase = []
    all_edges_list = []

    for phrase in list_phrases:

        sentence = [x.lower() for x in phrase]
        bigrams = zip(sentence, sentence[1:])
        trigrams = zip(sentence, sentence[1:], sentence[2:])

        for word in sentence:
            if word in key_phrases_dic:
                list_keywords_phrase.append(word)

        for j in bigrams:
            big_str = j[0] + ' ' + j[1]
            if big_str in key_phrases_dic:
                list_keywords_phrase.append(big_str)

        for i in trigrams:
            trig_str = i[0] + ' ' + i[1] + ' ' + i[2]
            if trig_str in key_phrases_dic:
                list_keywords_phrase.append(trig_str)

        all_edges_list.append(list_keywords_phrase)
        list_keywords_phrase = []

    graph = generate_graph(all_edges_list)

    return graph

def pageRank(grafo, d, l, key_phrase_pj):
    resultado = {}
    invertido = {}

    for key, value in grafo.iteritems():
        if key not in invertido.keys():
            invertido[key] = []
        for reference in value:
            if reference not in invertido.keys():
                invertido[reference] = []
            invertido[reference].append(key)

    numPages = len(grafo)
    for key, value in grafo.iteritems():
        resultado[key] = 1/(numPages*1.0)

    N = (numPages*1.0)
    for times in range(1, l+1):
        anterior = resultado.copy()
        for page in resultado.keys():
            somatorio = 0
            for reference in grafo[page]:
                v = anterior[reference] / (len(invertido[reference])*1.0)
                somatorio += anterior[reference]/(len(invertido[reference])*1.0)
            resultado[page] = (1-d) * 1.0 / N + d * somatorio

    for key_phrase in resultado:
        if key_phrase == key_phrase_pj:
            result = resultado[key_phrase]

    return result

def sumPrior( nodes , key_phrases_values):
    result_sum = 0
    for value in nodes:
        result_sum += key_phrases_values[value]
    return result_sum

def keyPhraseOcurrenceLine(filename, key_phrases_list):
    key_phrases_occurrence_phrase = {}
    nr_phrase = 0
    list_phrases=[]
    with open(filename, 'r') as file:
        all_phrases = file.read().strip().split('.')
        for words in all_phrases:
            list_phrases.append(words.split())

        for phrase in list_phrases:
            nr_phrase += 1
            sentence = [x.lower() for x in phrase]
            bigrams = zip(sentence, sentence[1:])
            trigrams = zip(sentence, sentence[1:], sentence[2:])

            for word in sentence:
                if word in key_phrases_list:
                    key_phrases_occurrence_phrase.setdefault(word, []).append(nr_phrase)

            for j in bigrams:
                big_str = j[0] + ' ' + j[1]
                if big_str in key_phrases_list:
                    key_phrases_occurrence_phrase.setdefault(big_str, []).append(nr_phrase)

            for i in trigrams:
                trig_str = i[0] + ' ' + i[1] + ' ' + i[2]
                if trig_str in key_phrases_list:
                    key_phrases_occurrence_phrase.setdefault(trig_str, []).append(nr_phrase)

    return key_phrases_occurrence_phrase

def PageRanking(key, graph, key_phrases_values):
    d = 0.15
    PR_2 = 0
    weight = 0
    prior = 1.0
    sumprior = 1.0

    for item in key_phrases_values[key]:
        prior += item

    for p in graph[key]:
        for item in key_phrases_values[p]:
            sumprior += item

    PR_1 = (prior / sumprior)

    for i in graph[key]:
        weight += len(graph[i])

    for n in graph[key]:
        PR_2 += ((pageRank(graph, 0.15, 49, n) * len(graph[key])) / weight)

    PR = d * PR_1 + (1 - d) * PR_2
    return PR

def get_key_values(file):
    content_all_files = []
    content = codecs.open(file, encoding='utf-8', errors='replace')
    for line in content.readlines():
        content_all_files.append(line.lower().split('\n')[0])
    return content_all_files


def precision(retrieved, relevant):
    if len(retrieved) == 0:
        print "Resultado foi: 0"
        return 0
    else:
        precision_result = float((len(list(set(retrieved) & set(relevant))))) / float((len(retrieved)))
        print "Precision: ", precision_result
        return precision_result

def recall(a, r):
    if len(r) == 0:
        print "Resultado foi: 0"
        return 0
    else:
        recall_result = float((len(set(iter(a)) & set(iter(r))))) / float((len(r)))
        print "Recall: ", recall_result
        return recall_result


def f1(pr, re):
    if re + pr == 0:
        print "Resultado foi: 0"
        return 0
    else:
        f1_result = float(((2 * re * pr) / (re + pr)))
    print "f1 score: ",  f1_result
    return f1

def list_filenames():
    f = []
    for (dirpath, dirnames, filenames) in walk('NLM_500/documents'):
        f.extend(filenames)
    return f

def initializer():
    PR={}
    train_data = read_files_text('NLM_500/documents/')
    prec_mean = []
    for name in list_filenames():
        if '.key' in name:
            r =get_key_values('NLM_500/documents/' + name )
            name = name[:-3] + 'txt'
            key_phrase_dic, key_phrases_values, key_phrases_list = score_candidates('NLM_500/documents/' + name, train_data)
            print "Nome do ficheiro", name
            list_phrases = phrases_spliter('NLM_500/documents/' + name)
            graph = create_graph(list_phrases, key_phrase_dic)
            key_phrases_ocurrances = keyPhraseOcurrenceLine('NLM_500/documents/' + name, graph)
            for key in graph:
                PR[key] = PageRanking(key, graph, key_phrases_ocurrances)
            prec = precision(PR, r)
            re = recall(PR, r)
            f1(prec, re)
            prec_mean += [prec]
            print PR
    temp=0
    for val in prec_mean:
        temp += val
    print('Mean Precision', temp/len(prec_mean))
    re = recall(PR, r)
    f1(prec, re)


initializer()
