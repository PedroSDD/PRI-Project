import codecs
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from operator import itemgetter
import logging
logging.basicConfig()

def read_files_text(path):
    content_all_files = []
    for file in os.listdir(path):
        if file.endswith(".txt"):
            content = codecs.open(os.path.join(path, file), encoding='utf-8', errors='replace')
            content_all_files.append(content.read().lower())
    return content_all_files


def score_candidates(filename, train):

    file_name = open(filename)
    vectorizer = TfidfVectorizer(use_idf=False, stop_words='english', ngram_range=(1, 3))
    vectorizer.fit_transform(train)
    my_doc_tfidf = vectorizer.transform(file_name)

    feature_names = vectorizer.get_feature_names()
    dense = my_doc_tfidf.todense()
    densed_idf = dense[0].tolist()[0]
    scores = zip(range(0, len(densed_idf)), densed_idf)
    feature_sorted = sorted(scores, key=itemgetter(1))[-5:]

    feature_name_sorted = []
    key_phrases_dic = {}

    for pair in feature_sorted:
        feature_name_sorted.append(feature_names[pair[0]])

    for word in feature_name_sorted:
        key_phrases_dic[word] = 0

    return key_phrases_dic

def phrases_spliter(filename):
    list_phrases = []
    with open(filename, 'r') as file:
        all_phrases = file.read().strip().split('.')

        for words in all_phrases:
            list_phrases.append(words.split())
        return list_phrases

def check_keys(key_word, graph):
    for key in graph:
        if key_word in graph:
            return True
    return False


def create_graph(list_phrases, key_phrases_dic):
    graph = {}
    list_keywords_phrase = []
    list_keywords_phrase_bi = []
    list_keywords_phrase_tri = []


    for phrase in list_phrases:

        sentence = [x.lower() for x in phrase]
        bigrams = zip(sentence, sentence[1:])
        trigrams = zip(sentence, sentence[1:], sentence[2:])

        for word in sentence:
            if word in key_phrases_dic:
                list_keywords_phrase.append(word)
                if check_keys(word, graph) == False:
                    graph[word] = list_keywords_phrase



        for j in bigrams:
            big_str = j[0] + ' ' + j[1]
            if big_str in key_phrases_dic:
                list_keywords_phrase_bi.append(big_str)
                if check_keys(big_str, graph) == False:
                    graph[big_str] = list_keywords_phrase_bi

        for i in trigrams:
            trig_str = i[0] + ' ' + i[1] + ' ' + i[2]
            if trig_str in key_phrases_dic:
                list_keywords_phrase_tri.append(trig_str)
                if check_keys(trig_str, graph) == False:
                    graph[trig_str] = list_keywords_phrase_tri

        list_keywords_phrase = []
        list_keywords_phrase_bi = []
        list_keywords_phrase_tri = []

    return graph

def pageRank(grafo, d, l):
    resultado = {}

    invertido = {}
    for key, value in grafo.iteritems():

        if key not in invertido.keys():
            invertido[key] = []
        for reference in value:
            if reference not in invertido.keys():
                invertido[reference] = []
            invertido[reference].append(key)

    numPages = len(grafo)
    for key, value in grafo.iteritems():
        resultado[key] = 1/(numPages*1.0)

    N = (numPages*1.0)
    for times in range(1, l+1):
        anterior = resultado.copy()
        for page in resultado.keys():

            somatorio = 0
            for reference in grafo[page]:
                v = anterior[reference] / (len(invertido[reference])*1.0)
                somatorio += anterior[reference]/(len(invertido[reference])*1.0)
            resultado[page] = (1-d) * 1.0 / N + d * somatorio
    print resultado
    return resultado


def initializer():
    train_data=read_files_text('NLM_500/documents/')
    key_phrase_dic = score_candidates('10933267.txt', train_data)
    list_phrases = phrases_spliter('10933267.txt')
    graph = create_graph(list_phrases, key_phrase_dic)
    pageRank(graph, 1, 1)

initializer()
